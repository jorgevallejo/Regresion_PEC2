---
title: "Regresión, modelos y métodos"
subtitle: "Prueba de evaluación continua 2"
author: "Jorge Vallejo Ortega"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  html_document:
     toc: true
# pdf_document:
#   number_sections: true
# toc: true
# extra_dependencies: ["float"]
# urlcolor: blue
# header-includes:
#   - \renewcommand{\contentsname}{Índice}
# - \usepackage{float}

# Next code for knitting both types of documents automatically comes from https://stackoverflow.com/questions/39662365/knit-one-markdown-file-to-two-output-files/53280491#53280491
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
                    output_format = NULL,
                    output_dir = "results",
                    output_file = "vallejo_jorge_Reg_PEC2.html") })
# And:
# https://stackoverflow.com/a/46007686/10647267

# bibliography: references.bib
---
  
```{r setup, include=FALSE}
# knitr options

# Do not display code in output document
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      fig.align = "center")
```

```{r estructura de directorios, results='hide', include=FALSE}
# 'data' contains raw source data.
# 'intermediateData' contains .RData objects with processed data.
# 'results' stores the final report files.

directories <- c("data", "results", "intermediateData", "images")

# Create directories
lapply(directories, function(x){
  if (!(dir.exists(x))){
    dir.create(x)
  }
})
```

```{r delete results files, eval= FALSE, include=FALSE}
# Run this chunk ONLY if you want to re-do
# the complete the report FROM THE ORIGINAL DATA.
# Remember that the .RData files are there to
# avoid unnecesarily redoing of long data processing.

directories <- c("results/", "intermediateData/", "images/")

file.remove(
  # Create a character vector of relative paths
  # to all files in the variable directories
  list.files(path = directories,
             all.files = TRUE,
             full.names = TRUE,
             recursive = TRUE)
)
```

```{r libraries, include=FALSE}
# Load packages
library(knitr)
# library(tseries)
library(faraway)
# library(car)
library(MASS) # for ridge regression
# library(dae)
library(corrplot) # Correlation plots
library(pls) # Principal component regression
library(glmnet) # for cv.glmnet
```

# Ejercicio 1. Base de datos `diabetes.csv`

Como preparación al ejercicio, cargaremos los datos a memoria y examinaremos las características y estructura de los mismos.

```{r read data}
# Reading and loading data from file diabetes.txt
diabetes <- read.table("./data/diabetes.txt",
           header = TRUE
           )

kable(
  head(diabetes),
  caption = "Ejemplo de las primeras seis observaciones en la base de datos `diabetes`.",
  align = 'c'
)
```

Si examinamos la estructura de la base de datos:
```{r}
str(diabetes)
```

Vemos que se compone de 377 observaciones y 11 variables. Todas las variables han sido correctamente identificadas como numéricas.

Finalmente, una forma rápida de buscar valores anómalos y valores ausentes es mediante el resumen numérico:
```{r}
summary(diabetes)
```

No parece haber valores anómalos (entendidos como exageradamente elevados o exageradamente pequeños) en ninguna de las variables. Por lo menos, en las variables _age_, _cintura_, _cadera_, _altura_ y _peso_; que tienen máximos y mínimos claramente dentro del rango esperable. En el resto de variables es más difícil de asegurar, ya que no estoy familiarizado con ellas e incluso algunas desconozco a qué se refieren. En cualquier caso, no parece haber ningún valor que destaque. Tampoco se ha detectado que falte ningún valor.

Finalmente, podríamos incluir un resumen gráfico de las variables para hacernos una idea de la distribución de valores en cada una:
```{r resumen_grafico, fig.cap="Gráficos de densidad mostrando la distribución de valores para cada una de las variables. Algunas de las variables, como _glyhb_, _stab.glu_ y _bp.1s_ parecen estar fuertemente sesgadas hacia valores bajos."}
par(mfrow=c(3,4))
for (i in 1:ncol(diabetes)){
plot(density(diabetes[,i]),
     main = colnames(diabetes[i]),
     xlab = "", ylab = "", yaxt = 'n')
}
```


## Ejercicio 1a. Ajustar un modelo lineal para explicar la variable hemoglobina glicosilada en función del resto de variables de la base de datos. Como el número de variables predictoras es elevado, investigar la posible multicolinealidad de este modelo de regresión.

Como primer paso, vamos a ajustar un modelo lineal con todas las variables predictoras

```{r hemo_mod1}
hemo_mod <- lm(glyhb ~ ., data = diabetes)
summary(hemo_mod)
```

Llama la atención lo pequeños que son los valores de los coeficientes de los predictores. Esto puede deberse a que los valores que adoptan las variables predictoras son uno o dos órdenes de magnitud superiores a los valores que adopta la variable respuesta. Para trabajar con unos coeficientes que sean menos confusos a la vista, cambiaremos la escala de la respuesta multiplicándola por 10 y ajustando los datos a un nuevo modelo:
```{r hemo_mod2}
hemo_mod <- lm(I(glyhb*10) ~ ., data = diabetes)
summary(hemo_mod)
```

Comprobamos que, efectivamente, los coeficientes (y los errores estándar) de los predictores son diez veces mayores que en el modelo anterior, lo que los hace más cómodos a ojos humanos. Sin embargo el resto de valores (estadísticos, p-valores, R^2) permanecen inalterados.

Si seguimos examinando el sumario de los tests estadísticos a partir del modelo, vemos que el valor de R^2 ajustado no es pequeño, lo que apunta a que el modelo recoge gran parte de la variación en los datos. Sin embargo, tan sólo dos de las variables predictoras es significativa. Eso podría significar que no afectan a la variable respuesta o podría ser un síntoma de **colinealidad**. La colinealidad (o multicolinealidad) es la situación en la que una variable predictora es una combinación lineal de otras. Dicho de otra forma, tenemos varios predictores que están midiendo lo mismo y, por tanto, no son ortogonales entre ellos.

Una forma rápida de comprobar qué variables se correlacionan entre ellas es representarlas por pares en gráficas de nubes de puntos:
```{r hemo_pairs}
pairs(diabetes[, -1])
```

A primera vista, las correlaciones más obvias son _cintura-cadera_, _cintura-peso_ y _cadera-peso_. Nada sorprendente si tenemos en cuenta dónde se acumula la mayor proporción de grasa corporal. Otra correlación que parece clara es _bp.1s-bp.1d_. Correlaciones más difusas podrían ser _altura-peso_, _chol-stab.glu_ y _chol-hdl_.

También podemos usar algo más preciso, como una **matriz de correlaciones**. Debido a la cantidad de variables, nos resultará más sencillo si la representamos en un gráfico de tipo mapa de calor:
```{r hemo_corr, fig.cap='Matriz de correlaciones. El color azul indica correlación positiva, el color rojo correlación negativa. El tamaño de los círculos y la saturación del color son proporcionales al grado de correlación.'}
corrplot::corrplot(
round(cor(diabetes[, -1]),2),
type = 'upper',
diag = FALSE,
addCoef.col = "white",
tl.srt = 45
)
```

En este gráfico vemos fácilmente que casi todos los pares de variables muestran algún tipo de correlación. Destacan, como veíamos antes, los pares de medidas físicas directamente relacionados con la acumulación de grasa ( _cintura-peso_, _cintura-cadera_ y _cadera-peso_), y en menor medida los pares _bp.1d-bp.1s_ y _age-bp.1s_. Si pensamos que variables muy correlacionadas están básicamente midiendo lo mismo, podemos simplificar el modelo eligiendo una única variable de cada grupo de variables relacionadas. Por ejemplo, para el grupo _cintura_ - _peso_ - _cadera_ podemos elegir peso (la más fácil de medir); y del par _bp.1d-bp.1s_ quedarnos sólo con _bp.1d_.

Si ajustamos los datos al nuevo modelo:
```{r}
# Modelo sin cintura, cadera, ni bp.1s
# Multiplicamos glyhb por 10 para obtener coeficientes más legibles
hemo_mod_simp <- lm(I(glyhb*10) ~ chol + stab.glu + hdl + age + bp.1d + altura + peso, data = diabetes)
summary(hemo_mod_simp)
```

El modelo simplificado tiene un ajuste casi idéntico al del modelo completo según el valor de R^2 ajustado. La significatividad de los predictores dentro del modelo no parece haber cambiado, excepto por _age_, aunque ya tenía un p-valor un poco por encima de 0.05 en el modelo completo. El único aspecto en el que se aprecian grandes cambios es en los valores de los coeficientes de los predictores que no eran significativos en el modelo completo.


## 1b. Sabemos que en lugar de altura y peso, el IMC es un mejor indicador de obesidad y por otro lado, la ratio cintura/cadera es también mejor predictor que cintura y cadera por separado. Calcular estas variables y ajustar un modelo incluyéndolas en lugar de las originales y comparar ambos modelos. ¿Es posible un contraste para decidir entre ambos modelos?

En primer lugar, calcularemos el IMC (masa/estatura^2) y la ratio cintura-cadera y las añadiremos al resto de datos:
```{r}
## IMC
# We divide altura by 100 to change its units to meters
# because it is usually encoded that way for IMC
diabetes$imc <- diabetes$peso/(diabetes$altura/100)^2

## Ratio cintura/cadera
diabetes$ratiocc <- diabetes$cintura/diabetes$cadera
```

Y en segundo lugar, ajustamos los datos al modelo que tiene en cuenta el IMC y la ratio cintura/cadera:
```{r}
hemo_mod_imc <- lm(I(glyhb*10) ~ chol + stab.glu + hdl + age + bp.1s + bp.1d + ratiocc + imc, data = diabetes)
summary(hemo_mod_imc)
```

¿Cómo decidir entre ambos modelos? En primer lugar, comprobaremos si son equivalentes. A primera vista, el valor de R^2 ajustado es muy parecido:
```{r}
c('modelo original' = summary(hemo_mod)$adj.r.squared, 'modelo modificado' = summary(hemo_mod_imc)$adj.r.squared)
```

Seguidamente contrastaremos ambos modelos mediante un test _F_. A primera vista podríamos pensar que los modelos no están anidados porque las variables _IMC_ y _ratio_ no están en el modelo completo, y que tal contraste no es posible. Sin embargo, las variables _IMC_ y _ratio_ son combinaciones lineales de variables que sí están en el modelo completo, por tanto el contraste mediante un test _F_ tiene sentido.

Si los comparamos mediante ANOVA:
```{r}
anova(hemo_mod_imc, hemo_mod)
```

Vemos que, estadísticamente, ambos modelos son equivalentes (p-valor > 0.05). No parece haber razón para elegir uno sobre el otro.


## 1c. A partir del modelo mejorado del apartado anterior, ajustar un modelo de regresión con componentes principales y determinar el número de componentes óptimo que minimice el error cuadrático medio de predicción (o su raíz cuadrada).

Volvemos primeramente a examinar los valores de las variables para determinar si es necesario realizar algún tipo de escalado:

```{r}
# database with imc and ratiocc minus the variables
# included in these last variables
diabetes_modif <- diabetes[, ! names(diabetes) %in% c("cintura", "cadera", "altura", "peso")]
head(diabetes_modif)
```

Contemplamos diferencias de hasta tres órdenes de magnitud entre algunas variables (e.g. entre _chol_ y _ratiocc_). Esta diferencia haría que las variables con valores mayores estuvieran sobrerrepresentadas en las componentes principales al tener mayor varianza. Para evitar ese sesgo convertiremos los valores de todas las variables a unidades estándar (restando su media y dividiendo por su desviación estándar); y calcularemos las componentes principales a partir de esos valores:

```{r}
PC_diabetes_modif <- prcomp(diabetes_modif[, 2:9], scale = TRUE)
summary(PC_diabetes_modif)
```

Vemos que en este caso la varianza está muy repartido. Necesitamos hasta 5 componentes principales para reunir al menos el 80% de la varianza, y 7 para reunir más del 90% de la misma.

Por curiosidad, veamos cuál es la aportación de las variables a cada componente principal:
```{r}
round(PC_diabetes_modif$rotation, digits = 2)
```

Para decidir cuántas componentes principales incluir como predictores en el modelo, buscaremos la combinación de predictores que ofrece predicciones con menor RMSE (raíz del error cuadrático medio). El método que usaremos para dividir las observaciones del set de datos en entrenamiento y validación de los modelos será el de validación cruzada (usando 10 segmentos aleatorios de los datos). Utilizaremos funciones del paquete `pls` del lenguaje **R**:

```{r, fig.cap="Representación de la raíz del error cuadrático medio según el número de componentes usado en la predicción de la variable glyhb. El objetivo es averiguar cuántos componentes ofrecen el menor error medio."}
# Regresion sobre componentes principales
set.seed(12021021)
PC_reg <- pcr(glyhb ~ ., data = diabetes_modif, 
              scale = TRUE, 
              validation = "CV", 
              ncomp = 8)
# Estimate the RMSEP by cross-validation
PC_reg_CV <- RMSEP(PC_reg, estimate = "CV")
# Components at which RMSEP in minimum
min_comp <- which.min(PC_reg_CV$val)
# Plot
plot(PC_reg_CV,
     main = "RMSEP de la predicción por número de componentes",
     xlab = "Número de componentes")
text(min_comp-1, # x coord is components
     y = round(PC_reg_CV$val[min_comp], digits = 2)*1.05, # y coord is RMSEP plus 10%
     labels = round(PC_reg_CV$val[min_comp],2)) # text is RMSEP value
```

Vemos con bastante claridad en el gráfico que `r min_comp-1`  es la cantidad de componentes con la que obtenemos mejores predicciones de los valores de la variable respuesta _glyhb_ en nuestro set de datos.

Finalmente, una vez decididas la cantidad de componentes principales óptima según nuestro set de datos, procedemos a reajustar el modelo de regresión:

```{r}
set.seed(5500)
PC_reg_mod <- pcr(glyhb ~ ., data = diabetes_modif, 
              scale = TRUE, 
              validation = "CV", 
              ncomp = min_comp-1)
```



## 1d. Con el mismo modelo, ajustar un modelo de regresión por mínimos cuadrados parciales (PLS) y determinar el número óptimo de componentes.

Aunque el método es diferente, los pasos que seguiremos serán similares a los del apartado anterior utilizando en esta ocasión la función `plsr()`:
```{r, fig.cap="Representación de la raíz del error cuadrático medio según el número de componentes usado en la predicción de la variable glyhb mediante diferentes modelos generados por regresión de mínimos cuadrados parciales. El objetivo es averiguar cuántos componentes ofrecen el menor error medio."}
set.seed(90012)
# Compute the PLS models
PLS_reg <- plsr(glyhb ~ ., data = diabetes_modif,
                scale = TRUE,
                validation = "CV")
# Estimate the RMSEP by cross-validation
PLS_reg_CV <- RMSEP(PLS_reg, estimate = "CV")
# Components at which RMSEP in minimum
min_comp <- which.min(PLS_reg_CV$val)
# Plot
plot(PLS_reg_CV,
     main = "RMSEP de la predicción por número de componentes",
     xlab = "Número de componentes")
text(min_comp-1, # x coord is components
     y = round(PLS_reg_CV$val[min_comp], digits = 2)*1.05, # y coord is RMSEP plus 10%
     labels = round(PLS_reg_CV$val[min_comp],2)) # text is RMSEP value
```

Vemos en el gráfico que ha quedado señalado en `r min_comp-1` componentes la cantidad con la que obtenemos mejores predicciones de los valores de la variable respuesta _glyhb_ en nuestro set de datos.


## 1e. Ajustar también un modelo de regresión RIDGE y, utilizando validación cruzada, determinar el parámetro óptimo.

El modelo de regresión ridge se diferencia del modelo de mínimos cuadrados en que utiliza un parámetro $\lambda$ que penaliza los estimadores $\hat \beta$ de los predictores, generando estimadores menores de los que se obtendrían por mínimos cuadrados. Es especialmente útil cuando la varianza en los datos es alta.

Para seleccionar el valor óptimo de $\lambda$ mediante validación cruzada usaremos la función `cv.glmnet()` (que además escala y centra los datos de forma automática):

```{r ridge tenfold cv}
# Sequence of lambda values to try
la_va <- 10^seq(2, -2, length = 100)
# Set train and test subsets of data
set.seed(391)
diabetes_train <- sample(1:nrow(diabetes_modif), nrow(diabetes_modif)/2) # half the data for each set
diabetes_test <- -diabetes_train

# Compute the lambda that gives a minimum mean cross-validated error
ridge_diabetes <- cv.glmnet(as.matrix(diabetes_modif[diabetes_train, -1]), # predictors)
                            diabetes_modif[diabetes_train, 1], #response
                            alpha = 0, # ridge penalty passed to glmnet function
                            lambda = la_va) # user-provided lambda values

###Notes###
# cv.glmnet expects a matrix of predictors, not a data frame
# https://stackoverflow.com/questions/8457624/r-glmnet-list-object-cannot-be-coerced-to-type-double
```

El valor de $\lambda$ que resulta en el menor error durante validación cruzada en el set de entrenamiento es `r round(ridge_diabetes$lambda.min, digits = 3)`, y el valor de ese error es `r ridge_diabetes$cvm[which.min(ridge_diabetes$cvm)]`.

Si probamos ese valor de $\lambda$ en el subset de validación:
```{r}
# Predict response from test subset
ridge_diabetes_predic <- predict(ridge_diabetes, #fitted glmnet object 
                                 s = ridge_diabetes$lambda.min, # lambda
                                 newx = as.matrix(diabetes_modif[diabetes_test, -1] # matrix of new values for prediction
                                                  ))

# Compute MSE error for test set
mean((ridge_diabetes_predic - diabetes_modif[diabetes_test, 1])^2)
```

Los errores de ambos sets son bastante parecidos.

Finalmente, volvemos a ajustar el modelo de regresión ridge sobre el conjunto completo de datos usando el valor de $\lambda$ elegido por validación cruzada y examinamos los coeficientes calculados para los predictores:
```{r}
# Fit the model
ridge_diabetes_full <- glmnet(as.matrix(diabetes_modif[, -1]), # predictores
                              diabetes_modif[, 1], # respuesta
                              alpha = 0, # ridge penalty
                              lambda = la_va) 

# Predict the outcomes
predict(ridge_diabetes_full,
        type = "coefficients", # compute the coefficients at values for s
        s = ridge_diabetes$lambda.min)[1:9,]
```

Parece que el predictor más influyente es la **ratio cintura-cadera**. Al resto de predictores se les asignan coeficientes uno y hasta dos órdenes de magnitud menores.

```{r ridge coef plot, fig.cap="Gráfica de valores de log(lambda) frente a coeficientes de los predictores. La línea roja representa el estimador del coeficiente de la variable `ratiocc`; la línea vertical intermitente marca el valor de log(lambda) que minimiza la media de errores al cuadrado (MSE)."}
plot(ridge_diabetes_full, xvar = 'lambda')
abline(v=log(ridge_diabetes$lambda.min), lty = 2)
```



## 1f. Ajustar por último un modelo de regresión LASSO y, con validación cruzada, hallar el parámetro óptimo.

El método de regresión _lasso_ "permite" eliminar variables del modelo de regresión, simplificándolo. Al igual que la regresión ridge, hace menguar los coeficientes estimados de los predictores pero, a diferencia de aquella, dichos coeficientes pueden llegar a ser igual a cero, efectivamente eliminando la variable del modelo. En general, el método _lasso_ es más útil cuando la respuesta depende de un pequeño número de predictores con coeficientes relativamente grandes y el resto de predictores tienen coeficientes pequeños o iguales a cero. El método _ridge_ es más adecuado cuando la respuesta depende de muchos predictores con coeficientes de tamaños similares.

Podemos utilizar la función `glmnet()` igual que hicimos en el apartado anterior para la regresión por el método ridge. En primer lugar, usaremos validación cruzada sobre el grupo de datos de entrenamiento para decidir el valor de $\lambda$ más adecuado:

```{r lasso tenfold cv}
set.seed(391)

# Compute the lambda that gives a minimum mean cross-validated error
lasso_diabetes <- cv.glmnet(as.matrix(diabetes_modif[diabetes_train, -1]), # predictors)
                            diabetes_modif[diabetes_train, 1], #response
                            alpha = 1, # lasso penalty passed to glmnet function
                            lambda = la_va) 

lasso_lambda <- c(ridge_diabetes$lambda.min, # Lambda value 
                  ridge_diabetes$cvm[which.min(ridge_diabetes$cvm)]) # Minimu average error

```

El valor de $\lambda$ que resulta en el menor error durante validación cruzada en el set de entrenamiento es `r round(lasso_lambda[1], digits = 3)`, y el valor de ese error es `r lasso_lambda[2]`.

Si probamos ese valor de $\lambda$ en el subset de validación:
```{r}
# Predict response from test subset
lasso_diabetes_predic <- predict(lasso_diabetes, #fitted glmnet object 
                                 s = lasso_lambda[1], # lambda
                                 newx = as.matrix(diabetes_modif[diabetes_test, -1] # matrix of new values for prediction
                                                  ))

# Compute MSE error for test set
mean((lasso_diabetes_predic - diabetes_modif[diabetes_test, 1])^2)
```

Los errores de ambos sets son bastante parecidos.

Finalmente, volvemos a ajustar el modelo de regresión lasso sobre el conjunto completo de datos usando el valor de $\lambda$ elegido por validación cruzada y examinamos los coeficientes calculados para los predictores:

```{r}
# Fit the model
lasso_diabetes_full <- glmnet(as.matrix(diabetes_modif[, -1]), # predictores
                              diabetes_modif[, 1], # respuesta
                              alpha = 1, # lasso penalty passed to glmnet function
                            lambda = la_va)

# Predict the outcomes
predict(lasso_diabetes_full,
        type = "coefficients", # compute the coefficients at values for s
        s = lasso_diabetes$lambda.min)[1:9,]
```

Vemos que dos de los ocho predictores tienen estimadores de coeficiente igual a cero, lo que los deja fuera del modelo final y **nos sugiere un modelo simplificado de seis variables predictoras**.

```{r lasso coef plot, fig.cap="Gráfica de valores de log(lambda) frente a coeficientes de los predictores. La línea roja representa el estimador del coeficiente de la variable `ratiocc`; la línea vertical intermitente marca el valor de log(lambda) que minimiza la media de errores al cuadrado (MSE)."}
plot(lasso_diabetes_full, xvar = 'lambda')
abline(v=log(lasso_diabetes$lambda.min), lty = 2)
```

Podemos ver en la gráfica cómo, a medida que aumenta lambda, los estimadores para los coeficientes van progresivamente igualándose a cero. Sin embargo, en la gráfica equivalente para la regresión por el métido ridge, los estimadores nunca llegan a igualarse a cero sino que sólo se aproximan a ese valor sin alcanzarlo.


# Ejercicio 2. Base de datos `gas_pressure.txt`

Como preparación al ejercicio, cargaremos los datos a memoria y examinaremos las características y estructura de los mismos.

```{r read gas pressure data}
# Reading and loading data from file gas_pressure.txt
gas_pressure <- read.table("./data/gas_pressure.txt",
           header = TRUE
           )

kable(
  head(gas_pressure),
  caption = "Ejemplo de las primeras seis observaciones en la base de datos `gas_pressure`.",
  align = 'c'
)
```

Si examinamos la estructura de la base de datos:
```{r}
str(gas_pressure)
```

Vemos que se compone de 40 observaciones y 2 variables. Todas las variables han sido correctamente identificadas como numéricas.

Finalmente, una forma rápida de buscar valores anómalos y valores ausentes es mediante el resumen numérico:
```{r}
summary(gas_pressure)
```

No parece haber valores anómalos (entendidos como exageradamente elevados o exageradamente pequeños) en ninguna de las variables. Aunque es difícil de asegurar, ya que no estoy familiarizado con estas variables. En cualquier caso, no parece haber ningún valor que destaque. Tampoco se ha detectado que falte ningún valor.

Finalmente, podríamos incluir un resumen gráfico de las variables para hacernos una idea de la distribución de valores en cada una:
```{r resumen_grafico gas_pressure, fig.cap="Gráficos de densidad mostrando la distribución de valores para cada una de las variables. Ambas variables tienen una distribución similar a la normal. Resulta notable que ambos gráficos presentan el mismo perfil."}
par(mfrow=c(1,2))
for (i in 1:ncol(gas_pressure)){
plot(density(gas_pressure[,i]),
     main = colnames(gas_pressure[i]),
     xlab = "", ylab = "", yaxt = 'n')
}
```






# Apéndice A: Código

El documento original en formato .Rmd, que incluye el código completo en lenguaje R usado para generar este informe, se puede consultar y descargar en el siguiente repositorio de Github:
[jorgevallejo/Regresion_PEC2](https://github.com/jorgevallejo/Regresion_PEC2)

# Apéndice B: Reproducibilidad {#apendiceB}
```{r session_info, include=TRUE, echo=TRUE, results='markup'}
sessionInfo() # For better reproducibility
```


# Referencias